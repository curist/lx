// Error handling utilities for the lx compiler
//
// Provides:
// - buildNodesIndex: Build complete nodeId -> node map from AST
// - resolveNodePosition: Map nodeId to source position (file:line:col)
// - formatError: Format a single error to string
// - printErrors: Print errors to stderr
// - collectErrors: Collect errors from compilation result

// ========================================
// Build Complete Nodes Index
// ========================================

fn buildNodesIndex(ast) {
  // Build a complete { nodeId: node } map by traversing entire AST
  // Guarantees O(1) lookup for any node ID
  let index = .{}

  fn traverse(node) {
    if !node or type(node) != "map" { return }

    // Register this node
    if node.id {
      index[node.id] = node
    }

    // Traverse all properties, filtering out metadata
    let props = keys(node)
    let metadataProps = .{
      id: true,
      type: true,
      line: true,
      col: true,
      endLine: true,
      endCol: true,
      filename: true,
      lexeme: true,
      anfSynthetic: true,
    }

    for let i = 0; i < len(props); i = i + 1 {
      let prop = props[i]

      // Skip metadata properties
      if metadataProps[prop] { continue }

      let child = node[prop]
      if child {
        if type(child) == "array" {
          for let j = 0; j < len(child); j = j + 1 {
            traverse(child[j])
          }
        } else if type(child) == "map" {
          // Only traverse map children (actual AST nodes)
          traverse(child)
        }
      }
    }
  }

  traverse(ast)
  index
}

// ========================================
// Fold Origins Across Passes
// ========================================

fn foldOrigins(nodeId, result) {
  // Generic origin folder that walks backwards through all passes
  // Returns the final (parser-level) node ID after following all origin chains

  if !nodeId or !result { return nil }

  let currentId = nodeId
  let maxDepth = 100  // Prevent infinite loops

  // If result has passOrder, walk it generically
  if result.passOrder and type(result.passOrder) == "array" {
    let order = result.passOrder

    // Walk from last executed pass backwards
    for let oi = len(order) - 1; oi >= 0; oi = oi - 1 {
      let name = order[oi]
      let pr = (result.passes and result.passes[name]) or nil
      let origin = pr and pr.origin

      if !origin { continue }

      let depth = 0
      for depth < maxDepth and origin[currentId] {
        currentId = origin[currentId]
        depth = depth + 1
      }
    }

    return currentId
  }

  // Fallback: legacy hard-coded origin walking (for compatibility)
  let depth = 0

  // Follow ANF origin if available
  if result.anfResult and result.anfResult.origin {
    for depth < maxDepth and result.anfResult.origin[currentId] {
      currentId = result.anfResult.origin[currentId]
      depth = depth + 1
    }
  }

  // Then follow lowered origin if available
  if result.lowerResult and result.lowerResult.origin {
    for depth < maxDepth and result.lowerResult.origin[currentId] {
      currentId = result.lowerResult.origin[currentId]
      depth = depth + 1
    }
  }

  currentId
}

// ========================================
// Resolve Node Position
// ========================================

fn resolveNodePosition(nodeId, result) {
  // Resolve nodeId to source position by:
  // 1. Following origin chains across all passes
  // 2. Looking up node in parser AST
  // 3. Extracting position info

  if !nodeId or !result { return nil }

  // Fold origins to get parser-level node ID
  let leafId = foldOrigins(nodeId, result)
  if !leafId { return nil }

  // Look up node in parser AST
  let node = nil
  if result.passes and result.passes.parse and result.passes.parse.ast {
    // New pipeline: use passes.parse.ast
    let index = buildNodesIndex(result.passes.parse.ast)
    node = index[leafId]
  } else if result.parseResult and result.parseResult.ast {
    // Legacy: use parseResult.ast
    let index = buildNodesIndex(result.parseResult.ast)
    node = index[leafId]
  }

  if !node { return nil }

  return .{
    filename: node.filename,
    line: node.line,
    col: node.col or 0,
  }
}

// ========================================
// Format Single Error
// ========================================

fn formatError(err, result) {
  // Format a single error to string
  // Handles both string errors (from parser) and object errors (from semantic phases)

  if type(err) == "string" {
    // Parser error - already formatted
    return err
  }

  if err.nodeId {
    // Semantic error - resolve nodeId to position
    let pos = resolveNodePosition(err.nodeId, result)
    if pos {
      let file = pos.filename
      let prefix = file and join([file, ":", pos.line, ":", pos.col], "") or
        join([pos.line, ":", pos.col], "")
      return join(["[", prefix, "] ", err.message], "")
    }

    // Fallback if position resolution fails
    return join(["[nodeId:", err.nodeId, "] ", err.message], "")
  }

  // Fallback for unknown error format
  err.message or str(err)
}

// ========================================
// Print Errors
// ========================================

fn printErrors(errors, result) {
  // Print all errors to stderr
  // errors: array of error objects or strings
  // result: compilation result (for resolving nodeIds)

  if !errors or len(errors) == 0 { return }

  for let i = 0; i < len(errors); i = i + 1 {
    let err = errors[i]
    groanln(formatError(err, result))
  }
}

// ========================================
// Collect Errors from Result
// ========================================

fn collectErrors(result) {
  // Collect all errors from a compilation result object
  // Returns array of errors from all phases

  let errors = []

  // New pipeline: prefer result.passOrder and result.passes
  if result.passOrder and result.passes and type(result.passOrder) == "array" {
    // Iterate through passes in execution order
    for let i = 0; i < len(result.passOrder); i = i + 1 {
      let passName = result.passOrder[i]
      let passResult = result.passes[passName]

      if passResult and passResult.errors {
        errors = concat(errors, passResult.errors)
      }
    }

    // Also collect from codegen/verify if present (post-pipeline passes)
    if result.codegenResult and result.codegenResult.errors {
      errors = concat(errors, result.codegenResult.errors)
    }

    if result.verifyResult and result.verifyResult.errors {
      errors = concat(errors, result.verifyResult.errors)
    }

    return errors
  }

  // Legacy fallback: collect from individual result fields
  // Note: parseErrors/lowerErrors are for backward compat
  if result.parseResult and result.parseResult.errors {
    errors = concat(errors, result.parseResult.errors)
  } else if result.parseErrors {
    errors = concat(errors, result.parseErrors)
  }

  if result.lowerResult and result.lowerResult.errors {
    errors = concat(errors, result.lowerResult.errors)
  } else if result.lowerErrors {
    errors = concat(errors, result.lowerErrors)
  }

  if result.resolveResult and result.resolveResult.errors {
    errors = concat(errors, result.resolveResult.errors)
  }

  if result.codegenResult and result.codegenResult.errors {
    errors = concat(errors, result.codegenResult.errors)
  }

  if result.verifyResult and result.verifyResult.errors {
    errors = concat(errors, result.verifyResult.errors)
  }

  errors
}

// ========================================
// Module Exports
// ========================================

.{
  buildNodesIndex: buildNodesIndex,
  foldOrigins: foldOrigins,
  resolveNodePosition: resolveNodePosition,
  formatError: formatError,
  printErrors: printErrors,
  collectErrors: collectErrors,
}
